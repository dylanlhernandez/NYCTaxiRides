{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn in the homework, make sure everything runs as expected. To do so, select **Kernel**$\\rightarrow$**Restart & Run All** in the toolbar above.  Remember to submit both on **DataHub** and **Gradescope**.\n",
    "\n",
    "Please fill in your name and include a list of your collaborators below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Dylan Hernandez\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: NYC Taxi Rides\n",
    "# Extras\n",
    "\n",
    "Put all of your extra work in here. Feel free to save figures to use when completing Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt #1\n",
    "\n",
    "For this model, the features I will use include pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, manhattan, day_of_week, and hour.\n",
    "\n",
    "In this attempt, I will use the training set from part 1, which is limited to all ride data from the month of January. I will also make sure to handle any outliers in test set by replacing their values with averages (DONE in pt4). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "\n",
    "The following code loads the training and validation data from part 1 into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data. \n",
    "data_file = Path(\"./\", \"cleaned_data.hdf\")\n",
    "train_df = pd.read_hdf(data_file, \"train\")\n",
    "val_df = pd.read_hdf(data_file, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable Pipeline for this attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from part 2\n",
    "def haversine(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    Compute haversine distance\n",
    "    \"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    average_earth_radius = 6371\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * average_earth_radius * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "# Copied from part 2\n",
    "def manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    Compute Manhattan distance\n",
    "    \"\"\"\n",
    "    a = haversine(lat1, lng1, lat1, lng2)\n",
    "    b = haversine(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "# Copied from part 2\n",
    "def bearing(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"\n",
    "    Compute the bearing, or angle, from (lat1, lng1) to (lat2, lng2).\n",
    "    A bearing of 0 refers to a NORTH orientation.\n",
    "    \"\"\"\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "# Copied from part 2\n",
    "def add_time_columns(df):\n",
    "    \"\"\"\n",
    "    Add temporal features to df\n",
    "    \"\"\"\n",
    "    df.is_copy = False # propogate write to original dataframe\n",
    "    df.loc[:, 'month'] = df['tpep_pickup_datetime'].dt.month\n",
    "    df.loc[:, 'week_of_year'] = df['tpep_pickup_datetime'].dt.weekofyear\n",
    "    df.loc[:, 'day_of_month'] = df['tpep_pickup_datetime'].dt.day\n",
    "    df.loc[:, 'day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "    df.loc[:, 'hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    df.loc[:, 'week_hour'] = df['tpep_pickup_datetime'].dt.weekday * 24 + df['hour']\n",
    "    return df\n",
    "\n",
    "# Copied from part 2\n",
    "def add_distance_columns(df):\n",
    "    \"\"\"\n",
    "    Add distance features to df\n",
    "    \"\"\"\n",
    "    df.is_copy = False # propogate write to original dataframe\n",
    "    df.loc[:, 'manhattan'] = manhattan_distance(lat1=df['pickup_latitude'],\n",
    "                                                lng1=df['pickup_longitude'],\n",
    "                                                lat2=df['dropoff_latitude'],\n",
    "                                                lng2=df['dropoff_longitude'])\n",
    "\n",
    "    df.loc[:, 'bearing'] = bearing(lat1=df['pickup_latitude'],\n",
    "                                   lng1=df['pickup_longitude'],\n",
    "                                   lat2=df['dropoff_latitude'],\n",
    "                                   lng2=df['dropoff_longitude'])\n",
    "    df.loc[:, 'haversine'] = haversine(lat1=df['pickup_latitude'],\n",
    "                                   lng1=df['pickup_longitude'],\n",
    "                                   lat2=df['dropoff_latitude'],\n",
    "                                   lng2=df['dropoff_longitude'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def select_columns(data, *columns):\n",
    "    return data.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_gm2(data, test=False):\n",
    "    X = (\n",
    "        data\n",
    "        \n",
    "        # Transform data\n",
    "        .pipe(add_time_columns)\n",
    "        .pipe(add_distance_columns)\n",
    "        \n",
    "        .pipe(select_columns,        \n",
    "              'pickup_longitude',  \n",
    "              'pickup_latitude',   \n",
    "              'dropoff_longitude', \n",
    "              'dropoff_latitude',\n",
    "              'manhattan',\n",
    "              'day_of_week',\n",
    "              'hour',\n",
    "             )\n",
    "    )\n",
    "    if test:\n",
    "        y = None\n",
    "    else:\n",
    "        y = data['duration']\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/data100/lib/python3.6/site-packages/pandas/core/generic.py:4388: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/srv/conda/envs/data100/lib/python3.6/site-packages/pandas/core/generic.py:4389: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train, y_train = process_data_gm2(train_df)\n",
    "X_val, y_val = process_data_gm2(val_df)\n",
    "guided_model_2 = lm.LinearRegression(fit_intercept=True)\n",
    "guided_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = guided_model_2.predict(X_train)\n",
    "y_val_pred = guided_model_2.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE function\n",
    "def mae(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates MAE from actual and predicted values\n",
    "    Input:\n",
    "      actual (1D array-like): vector of actual values\n",
    "      predicted (1D array-like): vector of predicted/fitted values\n",
    "    Output:\n",
    "      a float, the MAE\n",
    "    \"\"\"\n",
    "    \n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error:  265.554685801\n"
     ]
    }
   ],
   "source": [
    "assert 200 <= mae(y_val_pred, y_val) <= 300\n",
    "print(\"Validation Error: \", mae(y_val_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "This model can most definitely be improved. Although the validation error for this attempt is in between 200 and 300, my kaggle score 458.44820. Let's try something else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #2\n",
    "\n",
    "My model in the previous attempt used the a training set that only included data for the month of January. For this attempt, we will use the same features from before,as well as distances, but with data for months Feburary up to May."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load cleaned training data that includes months up to May."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all the SQL stuff first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from utils import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 2.1G Nov  7 04:43 /srv/db/taxi_2016_student_small.sqlite\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /srv/db/taxi_2016_student_small.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table taxi has 15000000 rows!\n",
      "3.34 s elapsed\n"
     ]
    }
   ],
   "source": [
    "DB_URI = \"sqlite:////srv/db/taxi_2016_student_small.sqlite\"\n",
    "TABLE_NAME = \"taxi\"\n",
    "\n",
    "sql_engine = create_engine(DB_URI)\n",
    "with timeit():\n",
    "    print(f\"Table {TABLE_NAME} has {sql_engine.execute(f'SELECT COUNT(*) FROM {TABLE_NAME}').first()[0]} rows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a cleaned training set that includes months up to May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reusing a lot of queries from part 1\n",
    "q1d_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {TABLE_NAME}\n",
    "            WHERE tpep_pickup_datetime\n",
    "                BETWEEN '2016-02-01' AND '2016-05-01'\n",
    "                AND record_id % 100 == 0\n",
    "            ORDER BY tpep_pickup_datetime\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3a_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM ({q1d_query})\n",
    "            WHERE abs(julianday(tpep_dropoff_datetime) -  julianday(tpep_pickup_datetime)) < 0.5\n",
    "\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bounding_condition(lat_l, lat_u, lon_l, lon_u):\n",
    "    return f\"\"\"\n",
    "            pickup_longitude <= {lon_u} AND\n",
    "            pickup_longitude >= {lon_l} AND\n",
    "            dropoff_longitude <= {lon_u} AND\n",
    "            dropoff_longitude >= {lon_l} AND\n",
    "            pickup_latitude <= {lat_u} AND\n",
    "            pickup_latitude >= {lat_l} AND\n",
    "            dropoff_latitude <= {lat_u} AND\n",
    "            dropoff_latitude >= {lat_l} \n",
    "            \"\"\"\n",
    "\n",
    "q3b_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM ({q3a_query})\n",
    "            WHERE {bounding_condition(40.63, 40.85, -74.03, -73.75)}\n",
    "\n",
    "            \"\"\"\n",
    "lat_l = 40.63\n",
    "lat_u = 40.85\n",
    "lon_l = -74.03\n",
    "lon_u = -73.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26 s elapsed\n"
     ]
    }
   ],
   "source": [
    "q3c_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM ({q3b_query})\n",
    "            WHERE passenger_count > 0\n",
    "\n",
    "            \"\"\"\n",
    "with timeit():\n",
    "    new_training_set = pd.read_sql_query(q3c_query, sql_engine)\n",
    "new_training_set['tpep_pickup_datetime'] = pd.to_datetime(new_training_set['tpep_pickup_datetime'])\n",
    "new_training_set['tpep_dropoff_datetime'] = pd.to_datetime(new_training_set['tpep_dropoff_datetime'])\n",
    "new_training_set['duration'] = new_training_set[\"tpep_dropoff_datetime\"]-new_training_set[\"tpep_pickup_datetime\"]\n",
    "new_training_set['duration'] = new_training_set['duration'].dt.total_seconds()\n",
    "new_training_set = new_training_set[new_training_set['duration'] < 12 * 3600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "all_m_train_df, all_m_val_df = train_test_split(new_training_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the data so we can access it in pt4 notebook\n",
    "Path(\"data/allmonthdata\").mkdir(parents=True, exist_ok=True)\n",
    "data_file = Path(\"data/allmonthdata\", \"allmonth_cleaned_data.hdf\") # Path of hdf file\n",
    "all_m_train_df.to_hdf(data_file, \"train2\") # Train data of hdf file\n",
    "all_m_val_df.to_hdf(data_file, \"val2\") # Val data of hdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_manhattan_hav_and_bearing_outliers(df):\n",
    "    \"\"\"\n",
    "    Remove any outliers (3 sds above mean) in manhattan, haversine and bearing column.\n",
    "    \"\"\" \n",
    "    manhattan_avg = df.loc[:,'manhattan'].mean()\n",
    "    manhattan_sd = df.loc[:,'manhattan'].std()\n",
    "    df['z_scores'] = pd.Series([(i-manhattan_avg)/manhattan_sd for i in df['manhattan']])\n",
    "    \n",
    "    haversine_avg = df.loc[:,'haversine'].mean()\n",
    "    haversine_sd = df.loc[:,'haversine'].std()\n",
    "    df['z_scores_hav'] = pd.Series([(i-haversine_avg)/haversine_sd for i in df['haversine']])\n",
    "    \n",
    "    bearing_avg = df.loc[:,'bearing'].mean()\n",
    "    bearing_sd = df.loc[:,'bearing'].std()\n",
    "    df['z_scores_bea'] = pd.Series([(i-bearing_avg)/bearing_sd for i in df['bearing']])\n",
    "    df = df[(df['z_scores'] > -3) & (df['z_scores'] < 3) & (df['z_scores_hav'] > -3) & (df['z_scores_hav'] < 3) & (df['z_scores_bea'] > -3) & (df['z_scores_bea'] < 3)]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_gm3(data, test=False):\n",
    "    if test:\n",
    "        y = None    \n",
    "        X = (\n",
    "            data\n",
    "        \n",
    "            # Transform data\n",
    "            .pipe(add_time_columns)\n",
    "            .pipe(add_distance_columns)\n",
    "\n",
    "        \n",
    "            .pipe(select_columns, \n",
    "                  'passenger_count',\n",
    "                  'pickup_longitude',  \n",
    "                  'pickup_latitude',   \n",
    "                  'dropoff_longitude', \n",
    "                  'dropoff_latitude',\n",
    "                  'month',\n",
    "                  'day_of_week',\n",
    "                  'hour',\n",
    "                  'manhattan',\n",
    "                  'haversine',\n",
    "                  'bearing'\n",
    "                 )\n",
    "        )\n",
    "    else:\n",
    "        X = (\n",
    "            data\n",
    "        \n",
    "            # Transform data\n",
    "            .pipe(add_time_columns)\n",
    "            .pipe(add_distance_columns)\n",
    "            .pipe(remove_manhattan_hav_and_bearing_outliers)\n",
    "\n",
    "        \n",
    "            .pipe(select_columns, \n",
    "                  'passenger_count',\n",
    "                  'pickup_longitude',  \n",
    "                  'pickup_latitude',   \n",
    "                  'dropoff_longitude', \n",
    "                  'dropoff_latitude',\n",
    "                  'month',\n",
    "                  'day_of_week',\n",
    "                  'hour',\n",
    "                  'manhattan',\n",
    "                  'haversine',\n",
    "                  'bearing'\n",
    "                 )\n",
    "        )      \n",
    "        \n",
    "        y = remove_manhattan_hav_and_bearing_outliers(data)['duration']\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/data100/lib/python3.6/site-packages/pandas/core/generic.py:4388: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/srv/conda/envs/data100/lib/python3.6/site-packages/pandas/core/generic.py:4389: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "X_train2, y_train2 = process_data_gm3(all_m_train_df)\n",
    "X_val2, y_val2 = process_data_gm3(all_m_val_df)\n",
    "guided_model_3 = lm.Ridge(fit_intercept=True)\n",
    "guided_model_3.fit(X_train2, y_train2)\n",
    "\n",
    "# Predict\n",
    "y_train_pred2 = guided_model_3.predict(X_train2)\n",
    "y_val_pred2 = guided_model_3.predict(X_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error:  250.593999008\n"
     ]
    }
   ],
   "source": [
    "assert 200 <= mae(y_val_pred2, y_val2) <= 300\n",
    "print(\"Validation Error: \", mae(y_val_pred2, y_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "You're almost done!\n",
    "\n",
    "Before submitting this assignment, ensure that you have:\n",
    "\n",
    "1. Restarted the Kernel (in the menubar, select Kernel$\\rightarrow$Restart & Run All)\n",
    "2. Validated the notebook by clicking the \"Validate\" button.\n",
    "\n",
    "Then,\n",
    "\n",
    "1. **Submit** the assignment via the Assignments tab in **Datahub** \n",
    "1. **Upload and tag** the manually reviewed portions of the assignment on **Gradescope**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
